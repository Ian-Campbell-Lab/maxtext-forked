{
  "version": "0.2.0",
  "configurations": [
    {
      "name": "Debug MaxText Decode",
      "type": "python",
      "request": "launch",
      "console": "integratedTerminal",
      "justMyCode": false,
      "python": "python3",
      "program": "${workspaceFolder}/MaxText/decode.py",
      "args": ["MaxText/configs/base.yml", 
               "run_name=runner_$(date +%Y-%m-%d-%H-%M)", 
               "base_output_directory=gs://test-maxtext-output",
               "dataset_path=gs://test-maxtext-dataset",
               "steps=2",
               "attention=dot_product",
               "enable_checkpointing=false"]
    },
    {
      "name": "Debug MaxText Train",
      "type": "python",
      "request": "launch",
      "console": "integratedTerminal",
      "justMyCode": false,
      "python": "python3",
      "program": "${workspaceFolder}/MaxText/train.py",
      "args": ["MaxText/configs/base.yml", 
               "run_name=runner_$(date +%Y-%m-%d-%H-%M)", 
               "base_output_directory=gs://test-maxtext-output",
               "dataset_path=gs://test-maxtext-dataset",
               "steps=2",
               "enable_checkpointing=false"]
    },
    {
      "name": "Debug MaxText Inference Microbenchmark Llama2-70b intmp",
      "type": "python",
      "request": "launch",
      "console": "integratedTerminal",
      "justMyCode": false,
      "python": "python3",
      "program": "${workspaceFolder}/MaxText/inference_microbenchmark.py",
      "args": [
        "MaxText/configs/v5e/inference/llama2_70b_test.yml",
        "allow_split_physical_axes=False",
        "ici_tensor_parallelism=4",
        "ici_autoregressive_parallelism=2",
        "checkpoint_is_quantized=True",
        "load_parameters_path=gs://msingh-bkt/checkpoints/quant_llama2-70b-chat/mlperf_070924/intmp_mp_scale",
        "quantization=intmp",
        "quant_cfg_path=MaxText/configs/quantization/mp_scale.json",
        "quantize_kvcache=True",
        "kv_quant_dtype=int8",
        "kv_quant_axis=heads_and_dkv",
        "per_device_batch_size=6",
        "model_name=llama2-70b",
        "tokenizer_path=assets/tokenizer.llama2",
        "weight_dtype=bfloat16",
        "async_checkpointing=false",
        "attention=dot_product",
        "reshape_q=True",
        "scan_layers=false",
        "max_prefill_predict_length=1024",
        "max_target_length=2048",
        "base_output_directory=gs://test-maxtext-output",
        "run_name=runner_$(date +%Y-%m-%d-%H-%M)", 
        "inference_microbenchmark_prefill_lengths=256",
        "inference_microbenchmark_stages=prefill,generate",
        "inference_microbenchmark_loop_iters=1",
        "prefill_cache_axis_order=0,2,1,3",
        "ar_cache_axis_order=0,2,1,3",
        "compute_axis_order=0,2,1,3",
      ]
    },
    {
      "name": "Debug MaxText Inference Microbenchmark Llama2-70b int8w",
      "type": "python",
      "request": "launch",
      "console": "integratedTerminal",
      "justMyCode": false,
      "python": "python3",
      "program": "${workspaceFolder}/MaxText/inference_microbenchmark.py",
      "args": [
        "MaxText/configs/v5e/inference/llama2_70b_test.yml",
        "allow_split_physical_axes=False",
        "ici_tensor_parallelism=4",
        "ici_autoregressive_parallelism=2",
        "checkpoint_is_quantized=True",
        "load_parameters_path=gs://msingh-bkt/checkpoints/quant_llama2-70b-chat/mlperf_070924/int8w_",
        "quantization=int8w",
        "quantize_kvcache=True",
        "kv_quant_dtype=int8",
        "kv_quant_axis=heads_and_dkv",
        "per_device_batch_size=6",
        "model_name=llama2-70b",
        "tokenizer_path=assets/tokenizer.llama2",
        "weight_dtype=bfloat16",
        "async_checkpointing=false",
        "attention=dot_product",
        "reshape_q=True",
        "scan_layers=false",
        "max_prefill_predict_length=1024",
        "max_target_length=2048",
        "base_output_directory=gs://test-maxtext-output",
        "run_name=runner_$(date +%Y-%m-%d-%H-%M)", 
        "inference_microbenchmark_prefill_lengths=256",
        "inference_microbenchmark_stages=prefill,generate",
        "inference_microbenchmark_loop_iters=1",
        "prefill_cache_axis_order=0,2,1,3",
        "ar_cache_axis_order=0,2,1,3",
        "compute_axis_order=0,2,1,3",
      ]
    },
  ]
}